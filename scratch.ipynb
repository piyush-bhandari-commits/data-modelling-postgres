{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2 as pg \n",
    "import pandas as pd \n",
    "from queries import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_file(cur, log_filepath):\n",
    "    # all_files = []\n",
    "    # for root, dirs, files in os.walk(filepath):\n",
    "    #     files = glob.glob(os.path.join(root, '*.json'))\n",
    "    #     for f in files:\n",
    "    #         all_files.append(os.path.abspath(f))\n",
    "\n",
    "    # num_files = len(all_files)\n",
    "    # # print('{} files found in {}'.format(num_files, filepath))\n",
    "    # for file_path in all_files: \n",
    "    \n",
    "    # loading data into \n",
    "    log_df = pd.read_json(log_filepath, lines=True)\n",
    "    \n",
    "    # create time dataframe and filter by next song\n",
    "    time_df = log_df[log_df['page']=='NextSong']\n",
    "    time = pd.to_datetime(time_df['ts'], unit='ms')\n",
    "\n",
    "    time_data = []\n",
    "\n",
    "    for timestamp in time:\n",
    "        time_data.append([timestamp, timestamp.hour, timestamp.day, timestamp.week, timestamp.month, timestamp.year, timestamp.day_name()])\n",
    "\n",
    "    df_time = pd.DataFrame.from_records(time_data, columns=('start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday'))\n",
    "\n",
    "    for i, row in df_time.iterrows():\n",
    "        cur.execute(time_table_insert, list(row))\n",
    "    \n",
    "    log_df.replace('', np.nan, inplace=True)\n",
    "    log_df.dropna(subset=['userId'], inplace=True)\n",
    "\n",
    "    user_df = log_df[['userId', 'firstName', 'lastName', 'gender', 'level']]\n",
    "\n",
    "    for i, row in user_df.iterrows():\n",
    "        cur.execute(user_table_insert, row)\n",
    "\n",
    "    for index, row in log_df.iterrows():\n",
    "        cur.execute(song_select, (row.song, row.artist, row.length))\n",
    "        results = cur.fetchone()\n",
    "\n",
    "        if results:\n",
    "            songid, artistid = results\n",
    "        else:\n",
    "            songid, artistid = None, None\n",
    "        \n",
    "        songplay_data = (index, pd.to_datetime(row.ts, unit='ms'), int(row.userId), row.level, songid, artistid, row.sessionId, row.location, row.userAgent)\n",
    "\n",
    "        cur.execute(songplay_table_insert, songplay_data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(cur, conn, filepath, func):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root, '*.json'))\n",
    "        for f in files:\n",
    "            all_files.append(os.path.abspath(f))\n",
    "\n",
    "    num_files = len(all_files)\n",
    "    print('{} files found in {}'.format(num_files, filepath))\n",
    "\n",
    "    for i, data_file in enumerate(all_files, 1):\n",
    "        func(cur, data_file)\n",
    "        conn.commit()\n",
    "        print(\"{} / {} files processed\".format(i, num_files))\n",
    "\n",
    "    return num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "30 files found in data/log_data\n",
      "1 / 30 files processed\n",
      "2 / 30 files processed\n",
      "3 / 30 files processed\n",
      "4 / 30 files processed\n",
      "5 / 30 files processed\n",
      "6 / 30 files processed\n",
      "7 / 30 files processed\n",
      "8 / 30 files processed\n",
      "9 / 30 files processed\n",
      "10 / 30 files processed\n",
      "11 / 30 files processed\n",
      "12 / 30 files processed\n",
      "13 / 30 files processed\n",
      "14 / 30 files processed\n",
      "15 / 30 files processed\n",
      "16 / 30 files processed\n",
      "17 / 30 files processed\n",
      "18 / 30 files processed\n",
      "19 / 30 files processed\n",
      "20 / 30 files processed\n",
      "21 / 30 files processed\n",
      "22 / 30 files processed\n",
      "23 / 30 files processed\n",
      "24 / 30 files processed\n",
      "25 / 30 files processed\n",
      "26 / 30 files processed\n",
      "27 / 30 files processed\n",
      "28 / 30 files processed\n",
      "29 / 30 files processed\n",
      "30 / 30 files processed\n"
     ]
    }
   ],
   "source": [
    "conn = pg.connect(\"host=127.0.0.1 dbname=sparkifydb user=postgres password=student\")\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "data = process_data(cur, conn, filepath='data/log_data', func=process_log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "cur.close()\n",
    "cur.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "data"
   ]
  }
 ]
}